Driver Gaze Mapping on Road

Analysing and mapping the driver gaze will play very important role in future when autonomous car dominates the road transport.
Even though the driver is not driving in case if autonomous cars, the driver needs to be attentive of the road.
DGAZE dataset and i-DAGAZE plays a very important role in decreasing the cost of the hardware used so that it can be used by common people just by utilizing simple camera along with the trained model.

After reading the paper and other papers related to driver gaze mapping I have come up with following proposals for extending the research and future work:

1.Extending mapping for the cluster of obstacles instead of single point or single object of observation.
In the research which is already been done, the mapping is being done for single object assuming that the driver is observing single object at a time. In reality, this is not the case. Human eye can take input of a certain range of space, not just a point.
Along with the clear vision of certain range, the driver will have a blurred of bigger range.
Thus considering this factor plays a prominent role in raising a false alarm.
All the objects coming in the range should be marked as "observed".

2.Varifying if the driver follows road rules.
i-DGAZE just maps if the driver has observed the signboards and signals or not. Along with that, a deeplearning model should be implimented which varifies if the driver is following the road rules and generate a warning if not.
The captured image of the signboard can be analysed by five different methods: color based methods, shape based methods, color and shape based methods, machine learning methods, and LIDAR based methods. Since we are just using camera for detection not LIDAR, we can use one of the first for methods for analysing the sign board. Traffic signal can be easily analysed by color based method. 

3.Detction of emotional state of the driver
A model should be trained to detect the emotional state of the driver using the images/video captured by the camera.
Emotional state plays a very prominent role in human actions. 
If the emotion(happy/sad/angry etc) level of the driver is above the threshold, there is a high chance of driver being inattentative.Also few emotions will influence the decision making ability of the driver.So, by detecting the emotion and analysing the vehicle movement, warning should be issued if there is something wrong.
There are eight universal facial expressions which include: neutral, happy, sadness, anger, contempt, disgust, fear, and surprise.If the driver is expressing either of them except neutral, it means there are high chances of him being inattentative. 
The facial expressions and emotions can be determined by using computer vision and neural networks.
The image/video captured by the camera are fed into image classification algorithm and then image is recognised. All the redundent data are removed using feature extraction. ORB and Color Gradient Histogram can be used to detect features.Machine uses corner detection to detect the features and and infer the contents of the image.

4. Checking driver attention
Using the the image/video captured of the driver, an algorithm must be implemented to warn if the driver is not observing the mirror or gazing at a same point for a long time(might be thinkin of something and inattentaive of road) or closed his eyes for more time than time required for blinking.
